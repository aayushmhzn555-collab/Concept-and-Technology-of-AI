{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DZhEH1jW0zix"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris, load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Custom Decision Tree (from scratch)"
      ],
      "metadata": {
        "id": "Aib4PUY-1sUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SimpleDecisionTree:\n",
        "    def __init__(self, max_depth=None):\n",
        "        self.max_depth = max_depth\n",
        "        self.root = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Build the decision tree using the training data.\"\"\"\n",
        "        self.root = self._grow_tree(X, y)\n",
        "\n",
        "    def _grow_tree(self, X, y, current_depth=0):\n",
        "        n_samples, n_features = X.shape\n",
        "        unique_labels = np.unique(y)\n",
        "\n",
        "        if len(unique_labels) == 1:\n",
        "            return {'label': unique_labels[0]}\n",
        "        if n_samples == 0 or (self.max_depth is not None and current_depth >= self.max_depth):\n",
        "            return {'label': np.bincount(y).argmax()}\n",
        "        best_gain = -np.inf\n",
        "        best_split = None\n",
        "        for feature_idx in range(n_features):\n",
        "            for threshold in np.unique(X[:, feature_idx]):\n",
        "                left_indices = X[:, feature_idx] <= threshold\n",
        "                right_indices = ~left_indices\n",
        "                left_labels, right_labels = y[left_indices], y[right_indices]\n",
        "\n",
        "                gain = self._compute_info_gain(y, left_labels, right_labels)\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    best_split = {\n",
        "                        'feature': feature_idx,\n",
        "                        'threshold': threshold,\n",
        "                        'left_indices': left_indices,\n",
        "                        'right_indices': right_indices\n",
        "                    }\n",
        "        if best_split is None:\n",
        "            return {'label': np.bincount(y).argmax()}\n",
        "        left_subtree = self._grow_tree(X[best_split['left_indices']], y[best_split['left_indices']], current_depth + 1)\n",
        "        right_subtree = self._grow_tree(X[best_split['right_indices']], y[best_split['right_indices']], current_depth + 1)\n",
        "\n",
        "        return {\n",
        "            'feature': best_split['feature'],\n",
        "            'threshold': best_split['threshold'],\n",
        "            'left': left_subtree,\n",
        "            'right': right_subtree\n",
        "        }\n",
        "\n",
        "    def _compute_info_gain(self, parent, left, right):\n",
        "        \"\"\"Calculate information gain from a potential split.\"\"\"\n",
        "        parent_entropy = self._calculate_entropy(parent)\n",
        "        left_entropy = self._calculate_entropy(left)\n",
        "        right_entropy = self._calculate_entropy(right)\n",
        "\n",
        "        weighted_entropy = (len(left)/len(parent)) * left_entropy + (len(right)/len(parent)) * right_entropy\n",
        "        return parent_entropy - weighted_entropy\n",
        "\n",
        "    def _calculate_entropy(self, y):\n",
        "        \"\"\"Compute entropy for a set of labels.\"\"\"\n",
        "        if len(y) == 0:\n",
        "            return 0\n",
        "        probabilities = np.bincount(y) / len(y)\n",
        "        return -np.sum(probabilities * np.log2(probabilities + 1e-9))\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict labels for multiple samples.\"\"\"\n",
        "        return [self._predict_one(sample, self.root) for sample in X]\n",
        "\n",
        "    def _predict_one(self, x, node):\n",
        "        \"\"\"Predict the label for a single sample.\"\"\"\n",
        "        if 'label' in node:\n",
        "            return node['label']\n",
        "        feature_val = x[node['feature']]\n",
        "        if feature_val <= node['threshold']:\n",
        "            return self._predict_one(x, node['left'])\n",
        "        else:\n",
        "            return self._predict_one(x, node['right'])\n"
      ],
      "metadata": {
        "id": "Aw_vzNga08RZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Iris dataset: Custom vs Scikit-Learn Decision Tree"
      ],
      "metadata": {
        "id": "O1kMpk6u11cQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iris_data = load_iris()\n",
        "features, labels = iris_data.data, iris_data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    features, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "my_tree = SimpleDecisionTree(max_depth=3)\n",
        "my_tree.fit(X_train, y_train)\n",
        "y_pred_my_tree = my_tree.predict(X_test)\n",
        "accuracy_my_tree = accuracy_score(y_test, y_pred_my_tree)\n",
        "\n",
        "sk_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "sk_tree.fit(X_train, y_train)\n",
        "y_pred_sk_tree = sk_tree.predict(X_test)\n",
        "accuracy_sk_tree = accuracy_score(y_test, y_pred_sk_tree)\n",
        "\n",
        "print(\"\\n--- Iris Dataset: Decision Tree Comparison ---\")\n",
        "print(f\"Custom Tree Accuracy: {accuracy_my_tree:.4f}\")\n",
        "print(f\"Scikit-learn Tree Accuracy: {accuracy_sk_tree:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnrDuxPd08Pk",
        "outputId": "bdbc7a38-0475-4058-f3a8-8ac5fb6e813c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Iris Dataset: Decision Tree Comparison ---\n",
            "Custom Tree Accuracy: 1.0000\n",
            "Scikit-learn Tree Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Ensemble Methods(Wine Dataset)"
      ],
      "metadata": {
        "id": "ZIf7N24q2A6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wine_data = load_wine()\n",
        "X_features, y_labels = wine_data.data, wine_data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_features, y_labels, test_size=0.2, random_state=42\n",
        ")\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "y_pred_dt = dt_model.predict(X_test)\n",
        "f1_dt_score = f1_score(y_test, y_pred_dt, average='macro')\n",
        "\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "f1_rf_score = f1_score(y_test, y_pred_rf, average='macro')\n",
        "\n",
        "print(\"\\n--- Ensemble Methods on Wine Dataset ---\")\n",
        "print(f\"Decision Tree F1 Score: {f1_dt_score:.4f}\")\n",
        "print(f\"Random Forest F1 Score: {f1_rf_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUI1RTry2BLo",
        "outputId": "f7da8a04-5444-40a6-b5a4-505511954c25"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Ensemble Methods on Wine Dataset ---\n",
            "Decision Tree F1 Score: 0.9425\n",
            "Random Forest F1 Score: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Hyperparameter Tuning (Random Forest Classifier)"
      ],
      "metadata": {
        "id": "LRJYxVQ42PaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_params = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "rf_grid = GridSearchCV(\n",
        "    estimator=RandomForestClassifier(random_state=42),\n",
        "    param_grid=rf_params,\n",
        "    scoring='f1_macro',\n",
        "    cv=3\n",
        ")\n",
        "rf_grid.fit(X_train, y_train)\n",
        "print(\"\\n--- Random Forest Hyperparameter Tuning ---\")\n",
        "print(\"Optimal Parameters:\", rf_grid.best_params_)\n",
        "print(f\"Best Cross-Validated F1 Score: {rf_grid.best_score_:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_U19s7S08OE",
        "outputId": "1668ebb8-89a6-412f-a2cc-392caa39d7a2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Random Forest Hyperparameter Tuning ---\n",
            "Optimal Parameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Best Cross-Validated F1 Score: 0.9863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Regression Models (Wine dataset features â†’ target continuous)"
      ],
      "metadata": {
        "id": "2-LujAfH2hwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_wine_reg = y_labels.astype(float)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_features, y_wine_reg, test_size=0.2, random_state=42\n",
        ")\n",
        "dt_model_reg = DecisionTreeRegressor(random_state=42)\n",
        "dt_model_reg.fit(X_train, y_train)\n",
        "y_pred_dt = dt_model_reg.predict(X_test)\n",
        "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
        "\n",
        "rf_model_reg = RandomForestRegressor(random_state=42)\n",
        "rf_model_reg.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model_reg.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "print(\"\\n--- Regression Models on Wine Dataset ---\")\n",
        "print(f\"Decision Tree MSE: {mse_dt:.4f}\")\n",
        "print(f\"Random Forest MSE: {mse_rf:.4f}\")\n",
        "\n",
        "rf_param_dist = {\n",
        "    'n_estimators': [50, 100, 200, 300],\n",
        "    'max_depth': [None, 5, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "rf_random_search = RandomizedSearchCV(\n",
        "    estimator=RandomForestRegressor(random_state=42),\n",
        "    param_distributions=rf_param_dist,\n",
        "    n_iter=5,\n",
        "    cv=3,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\n--- Random Forest Regressor Hyperparameter Tuning ---\")\n",
        "print(\"Optimal Parameters:\", rf_random_search.best_params_)\n",
        "print(f\"Best Cross-Validated Score (Neg MSE): {rf_random_search.best_score_:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-6KB1my2PNn",
        "outputId": "9fd25b95-2762-4ac8-ff69-3ed422fb7b59"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Regression Models on Wine Dataset ---\n",
            "Decision Tree MSE: 0.1667\n",
            "Random Forest MSE: 0.0648\n",
            "\n",
            "--- Random Forest Regressor Hyperparameter Tuning ---\n",
            "Optimal Parameters: {'n_estimators': 300, 'min_samples_split': 2, 'max_depth': 10}\n",
            "Best Cross-Validated Score (Neg MSE): -0.0509\n"
          ]
        }
      ]
    }
  ]
}